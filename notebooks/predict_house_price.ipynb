{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e9a2327-2ee0-4aa4-81bc-ec5faae8746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import KFold, cross_validate, HalvingGridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00025b8f-6447-40bd-b44e-08b4d89dec38",
   "metadata": {},
   "source": [
    "处理category列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9beb162-c558-4dca-a241-cb4916f1e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_category(string):\n",
    "    if not isinstance(string, str):\n",
    "        return \"others\"\n",
    "        \n",
    "    if \"housing/rent/apartment\" in string:\n",
    "        return \"apartment\"\n",
    "    elif \"housing/rent/commercial/retail\" in string:\n",
    "        return \"commercial/retail\"\n",
    "    elif \"housing/rent/home\" in string:\n",
    "        return \"home\"\n",
    "    elif \"housing/rent/condo\" in string:\n",
    "        return \"condo\"\n",
    "    elif \"housing/rent/short_term\" in string:\n",
    "        return \"short_term\"\n",
    "    else:\n",
    "        return \"others\"\n",
    "\n",
    "def apply_clean_category(X):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        s = X.iloc[:, 0]\n",
    "    else:\n",
    "        s = pd.Series(X.ravel())\n",
    "    cleaned_series = s.apply(clean_category)\n",
    "    return cleaned_series.to_frame()   # 保证二维\n",
    "\n",
    "category_cleaner = FunctionTransformer(apply_clean_category)\n",
    "\n",
    "category_pipe = Pipeline(steps=[\n",
    "    ('clean', category_cleaner),\n",
    "    ('one_hot', OneHotEncoder(handle_unknown='infrequent_if_exist', min_frequency=0.05))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a701e31-344d-4f83-b6ec-15f211196654",
   "metadata": {},
   "source": [
    "处理title和body列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "271abfc5-8a6e-40d3-b90b-d780e4a67db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.url_pat = re.compile(r'https?://\\S+|www\\.\\S+', re.I)\n",
    "        self.mail_pat = re.compile(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b')\n",
    "        self.ws_pat = re.compile(r'\\s+')\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        def norm(s):\n",
    "            if s is None: return \"\"\n",
    "            if isinstance(s, float) and np.isnan(s): return \"\"\n",
    "            return str(s)\n",
    "        cleaned = []\n",
    "        for t in X:\n",
    "            s = norm(t).lower()\n",
    "            s = self.url_pat.sub(\" \", s)\n",
    "            s = self.mail_pat.sub(\" \", s)\n",
    "            s = self.ws_pat.sub(\" \", s).strip()\n",
    "            cleaned.append(s)\n",
    "        return np.array(cleaned, dtype=object)\n",
    "\n",
    "class PriceMasker(BaseEstimator, TransformerMixin):\n",
    "    _pat = re.compile(\n",
    "        r'((?:\\$|€|£)\\s*\\d[\\d,\\.]*|\\b(?:usd|eur|gbp)\\b\\s*\\d[\\d,\\.]*|\\b\\d[\\d,\\.]{3,}\\b)\\s*(?:per\\s*(?:month|mo)|/mo|/m|/month)?',\n",
    "        flags=re.I\n",
    "    )\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        def norm(s):\n",
    "            if s is None: return \"\"\n",
    "            if isinstance(s, float) and np.isnan(s): return \"\"\n",
    "            return str(s)\n",
    "        return np.array([self._pat.sub(\" __PRICE__ \", norm(t)) for t in X], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "814df053-3e56-459f-9089-ec926b2e5b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsureTextDF(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, title_col=\"title\", body_col=\"body\"):\n",
    "        self.title_col = title_col\n",
    "        self.body_col = body_col\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        if hasattr(X, \"columns\"):  # pandas DataFrame\n",
    "            df = X\n",
    "            if self.title_col in df.columns and self.body_col in df.columns:\n",
    "                return df[[self.title_col, self.body_col]]\n",
    "            df2 = df.iloc[:, :2].copy()\n",
    "            df2.columns = [self.title_col, self.body_col]\n",
    "            return df2\n",
    "        arr = np.asarray(X, dtype=object)\n",
    "        if arr.ndim == 1:\n",
    "            arr = np.c_[arr, np.full_like(arr, \"\", dtype=object)]\n",
    "        if arr.shape[1] < 2:\n",
    "            arr = np.hstack([arr, np.full((arr.shape[0], 1), \"\", dtype=object)])\n",
    "        return pd.DataFrame({self.title_col: arr[:, 0], self.body_col: arr[:, 1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fd82172-df72-4989-9c3e-aa91452bb366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_body_pipeline(title_weight: float = 3.0) -> Pipeline:\n",
    "    title_pipe = Pipeline([\n",
    "        (\"clean\", SimpleTextCleaner()),\n",
    "        (\"mask_price\", PriceMasker()),\n",
    "        (\"tfidf_word\", TfidfVectorizer(\n",
    "            analyzer=\"word\",\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=3,\n",
    "            strip_accents=\"unicode\",\n",
    "            sublinear_tf=True,\n",
    "            smooth_idf=True,\n",
    "            stop_words=None,\n",
    "            max_features=15355\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    body_pipe = Pipeline([\n",
    "        (\"clean\", SimpleTextCleaner()),\n",
    "        (\"mask_price\", PriceMasker()),\n",
    "        (\"tfidf_char\", TfidfVectorizer(\n",
    "            analyzer=\"char\",\n",
    "            ngram_range=(3, 5),\n",
    "            min_df=3,\n",
    "            strip_accents=\"unicode\",\n",
    "            sublinear_tf=True,\n",
    "            smooth_idf=True,\n",
    "            max_features=210849\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    text_union = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"title\", title_pipe, \"title\"),\n",
    "            (\"body\",  body_pipe,  \"body\"),\n",
    "        ],\n",
    "        transformer_weights={\"title\": title_weight, \"body\": 1.0},\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    return Pipeline([\n",
    "        (\"ensure_df\", EnsureTextDF(\"title\", \"body\")),\n",
    "        (\"text_union\", text_union)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f98f3-f4e2-4607-ae12-ca8a2fa80e1b",
   "metadata": {},
   "source": [
    "处理amenities列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fae7972b-8ff5-49c3-8263-b275046c92fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_list = FunctionTransformer(\n",
    "    lambda X: [[t.strip() for t in str(s).split(',') if t.strip()] for s in np.asarray(X).reshape(-1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f20fd171-4b82-4484-972d-2bffe8bf99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.mlb.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.mlb.transform(X)\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        return self.mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52f48c26-e856-4106-a435-368cb505afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "amenities_pipe = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='Nothing')),\n",
    "    ('to_list', to_list),\n",
    "    ('multi_hot', MultiHotEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a45a2-4390-45f8-9030-e1391d6753ff",
   "metadata": {},
   "source": [
    "处理pets_allowed列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fac98ddb-45ae-427c-a835-bd937fd77959",
   "metadata": {},
   "outputs": [],
   "source": [
    "pets_allowed_pipe = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='No')),\n",
    "    ('to_list', to_list),\n",
    "    ('multi_hot', MultiHotEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7edf66-942f-4b28-beb6-cf0b54a1852b",
   "metadata": {},
   "source": [
    "处理bathrooms, bedrooms和square_feet列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afd7f708-680c-419a-98fe-ec127bc3790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BedBathSqftSimilarityImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    bedrooms:   面积相似 + bathrooms 相同 -> 平均\n",
    "    bathrooms:  面积相似 + bedrooms 相同 -> 平均\n",
    "    square_feet: bedrooms & bathrooms 相同 -> 平均\n",
    "    \"\"\"\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.train_ = X.copy()\n",
    "        self.global_means_ = self.train_.mean(numeric_only=True)\n",
    "        self.mean_by_bath_ = self.train_.dropna(subset=['bedrooms']).groupby('bathrooms')['bedrooms'].mean()\n",
    "        self.mean_by_bed_  = self.train_.dropna(subset=['bathrooms']).groupby('bedrooms')['bathrooms'].mean()\n",
    "        self.mean_by_pair_ = self.train_.dropna(subset=['square_feet']).groupby(['bedrooms','bathrooms'])['square_feet'].mean()\n",
    "        return self\n",
    "\n",
    "    def _knn_mean_by_area(self, df_cand, target_col, sf_value):\n",
    "        if df_cand.empty: return np.nan\n",
    "        cand = df_cand.dropna(subset=[target_col, 'square_feet'])\n",
    "        if cand.empty: return np.nan\n",
    "        if pd.isna(sf_value): return cand[target_col].mean()\n",
    "        d = (cand['square_feet'] - sf_value).abs()\n",
    "        topk = cand.loc[d.nsmallest(min(self.k, len(cand))).index, target_col]\n",
    "        return topk.mean() if len(topk) else np.nan\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        # ---- square_feet ----\n",
    "        mask_sf = X['square_feet'].isna() & X['bedrooms'].notna() & X['bathrooms'].notna()\n",
    "        for idx in X.loc[mask_sf].index:\n",
    "            key = (X.at[idx, 'bedrooms'], X.at[idx, 'bathrooms'])\n",
    "            if key in self.mean_by_pair_.index:\n",
    "                X.at[idx, 'square_feet'] = self.mean_by_pair_.loc[key]\n",
    "\n",
    "        # ---- bedrooms ----\n",
    "        mask_bed = X['bedrooms'].isna()\n",
    "        cand_bed = self.train_.dropna(subset=['bedrooms'])\n",
    "        for idx in X.loc[mask_bed].index:\n",
    "            bth, sf = X.at[idx,'bathrooms'], X.at[idx,'square_feet']\n",
    "            cand = cand_bed if pd.isna(bth) else cand_bed[cand_bed['bathrooms']==bth]\n",
    "            val = self._knn_mean_by_area(cand,'bedrooms',sf)\n",
    "            if pd.isna(val): val = self.mean_by_bath_.get(bth,np.nan)\n",
    "            if pd.isna(val): val = self.global_means_['bedrooms']\n",
    "            X.at[idx,'bedrooms']=val\n",
    "\n",
    "        # ---- bathrooms ----\n",
    "        mask_bth = X['bathrooms'].isna()\n",
    "        cand_bth = self.train_.dropna(subset=['bathrooms'])\n",
    "        for idx in X.loc[mask_bth].index:\n",
    "            bed, sf = X.at[idx,'bedrooms'], X.at[idx,'square_feet']\n",
    "            cand = cand_bth if pd.isna(bed) else cand_bth[cand_bth['bedrooms']==bed]\n",
    "            val = self._knn_mean_by_area(cand,'bathrooms',sf)\n",
    "            if pd.isna(val): val = self.mean_by_bed_.get(bed,np.nan)\n",
    "            if pd.isna(val): val = self.global_means_['bathrooms']\n",
    "            X.at[idx,'bathrooms']=val\n",
    "\n",
    "        # ---- square_feet 剩余兜底 ----\n",
    "        mask_sf2 = X['square_feet'].isna()\n",
    "        for idx in X.loc[mask_sf2].index:\n",
    "            key = (X.at[idx,'bedrooms'],X.at[idx,'bathrooms'])\n",
    "            val = self.mean_by_pair_.get(key,np.nan)\n",
    "            if pd.isna(val): val = self.global_means_['square_feet']\n",
    "            X.at[idx,'square_feet']=val\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2017b7a-d05f-4a27-9597-dbaa892017c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "coerce_bed_bath_sqft_to_numeric = FunctionTransformer(lambda X: X.apply(pd.to_numeric, errors='coerce'))\n",
    "\n",
    "bed_bath_sqft_pipe = Pipeline(steps=[\n",
    "    ('to_numeric', coerce_bed_bath_sqft_to_numeric),\n",
    "    ('similar_impute', BedBathSqftSimilarityImputer(k=8))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1221de6-2f75-4190-9538-611354d26d2d",
   "metadata": {},
   "source": [
    "处理cityname, state, latitude和longitude列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cc73283-b4b2-4d35-b985-5cb7b27846b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoCityStateImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "      state 缺失：优先用 cityname + (lat,lon)；再仅用 cityname 的众数；再用 (lat,lon) 最近邻；最后全局众数\n",
    "      cityname 缺失：先保证 state 已填；在该 state 内按 (lat,lon) 最近邻；再用该 state 的城市众数；最后全局众数\n",
    "      latitude/longitude 缺失：用相同 (state, cityname) 组内的均值；若无该组则用全局均值\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def _mode(s):\n",
    "        s = pd.Series(s).dropna()\n",
    "        return s.mode().iloc[0] if not s.empty else np.nan\n",
    "\n",
    "    @staticmethod\n",
    "    def _norm_key(x):\n",
    "        if pd.isna(x):\n",
    "            return pd.NA\n",
    "        return str(x).strip().lower()\n",
    "\n",
    "    @staticmethod\n",
    "    def _nearest_row(train_df, lat, lon, mask=None):\n",
    "        cand = train_df if mask is None else train_df[mask]\n",
    "        cand = cand.dropna(subset=['latitude','longitude'])\n",
    "        if cand.empty or pd.isna(lat) or pd.isna(lon):\n",
    "            return None\n",
    "        d2 = (cand['latitude'] - lat)**2 + (cand['longitude'] - lon)**2\n",
    "        return cand.loc[[d2.idxmin()]]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X, columns=['latitude','longitude','cityname','state'])\n",
    "        self.train_ = X.copy()\n",
    "\n",
    "        # 规范化键\n",
    "        self.train_['city_key']  = self.train_['cityname'].apply(self._norm_key)\n",
    "        self.train_['state_key'] = self.train_['state'].apply(self._norm_key)\n",
    "\n",
    "        # 全局统计\n",
    "        tmp = self.train_.copy()\n",
    "        tmp['latitude']  = pd.to_numeric(tmp['latitude'], errors='coerce')\n",
    "        tmp['longitude'] = pd.to_numeric(tmp['longitude'], errors='coerce')\n",
    "        self.global_state_mode_ = self._mode(self.train_['state'])\n",
    "        self.global_city_mode_  = self._mode(self.train_['cityname'])\n",
    "        self.global_lat_mean_   = tmp['latitude'].mean()\n",
    "        self.global_lon_mean_   = tmp['longitude'].mean()\n",
    "\n",
    "        # city -> state 众数\n",
    "        self.state_mode_by_city_ = (\n",
    "            self.train_.dropna(subset=['city_key','state'])\n",
    "                       .groupby('city_key')['state']\n",
    "                       .agg(self._mode)\n",
    "        )\n",
    "        # state -> city 众数\n",
    "        self.city_mode_by_state_ = (\n",
    "            self.train_.dropna(subset=['state_key','cityname'])\n",
    "                       .groupby('state_key')['cityname']\n",
    "                       .agg(self._mode)\n",
    "        )\n",
    "        # (state, city) -> (lat, lon) 均值\n",
    "        self.mean_latlon_by_pair_ = (\n",
    "            tmp.dropna(subset=['state_key','city_key'])\n",
    "               .groupby(['state_key','city_key'])[['latitude','longitude']]\n",
    "               .mean()\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X, columns=['latitude','longitude','cityname','state'])\n",
    "        X = X.copy()\n",
    "\n",
    "        # 辅助键\n",
    "        X['city_key']  = X['cityname'].apply(self._norm_key)\n",
    "        X['state_key'] = X['state'].apply(self._norm_key)\n",
    "\n",
    "        # ---------- 1) 填 state ----------\n",
    "        mask_state_na = X['state'].isna()\n",
    "        if mask_state_na.any():\n",
    "            for idx in X.index[mask_state_na]:\n",
    "                city_k = X.at[idx, 'city_key']\n",
    "                lat    = X.at[idx, 'latitude']\n",
    "                lon    = X.at[idx, 'longitude']\n",
    "                val = np.nan\n",
    "\n",
    "                # 用该 city 的 state 众数\n",
    "                if city_k is not pd.NA and city_k in self.state_mode_by_city_.index:\n",
    "                    val = self.state_mode_by_city_.loc[city_k]\n",
    "                    # 若有经纬度，尝试同 city 最近邻校正\n",
    "                    nr = self._nearest_row(self.train_, lat, lon, mask=(self.train_['city_key'] == city_k))\n",
    "                    if nr is not None and not nr['state'].isna().all():\n",
    "                        val = nr['state'].iloc[0]\n",
    "\n",
    "                # 仅用经纬度最近邻\n",
    "                if pd.isna(val):\n",
    "                    nr = self._nearest_row(self.train_, lat, lon)\n",
    "                    if nr is not None and not nr['state'].isna().all():\n",
    "                        val = nr['state'].iloc[0]\n",
    "\n",
    "                # 全局众数兜底\n",
    "                if pd.isna(val):\n",
    "                    val = self.global_state_mode_\n",
    "\n",
    "                X.at[idx, 'state'] = val\n",
    "                X.at[idx, 'state_key'] = self._norm_key(val)\n",
    "\n",
    "        # ---------- 2) 填 cityname ----------\n",
    "        mask_city_na = X['cityname'].isna()\n",
    "        if mask_city_na.any():\n",
    "            for idx in X.index[mask_city_na]:\n",
    "                st_k = X.at[idx, 'state_key']\n",
    "                lat  = X.at[idx, 'latitude']\n",
    "                lon  = X.at[idx, 'longitude']\n",
    "                val = np.nan\n",
    "\n",
    "                if st_k is not pd.NA:\n",
    "                    # 先在该 state 内最近邻\n",
    "                    nr = self._nearest_row(self.train_, lat, lon, mask=(self.train_['state_key'] == st_k))\n",
    "                    if nr is not None and not nr['cityname'].isna().all():\n",
    "                        val = nr['cityname'].iloc[0]\n",
    "                    # 再用该 state 的众数\n",
    "                    if pd.isna(val) and st_k in self.city_mode_by_state_.index:\n",
    "                        val = self.city_mode_by_state_.loc[st_k]\n",
    "\n",
    "                # 全局众数兜底\n",
    "                if pd.isna(val):\n",
    "                    val = self.global_city_mode_\n",
    "\n",
    "                X.at[idx, 'cityname'] = val\n",
    "                X.at[idx, 'city_key'] = self._norm_key(val)\n",
    "\n",
    "        # ---------- 3) 填经纬度 ----------\n",
    "        X['latitude']  = pd.to_numeric(X['latitude'], errors='coerce')\n",
    "        X['longitude'] = pd.to_numeric(X['longitude'], errors='coerce')\n",
    "\n",
    "        for idx in X.index:\n",
    "            st_k = X.at[idx, 'state_key']\n",
    "            ct_k = X.at[idx, 'city_key']\n",
    "\n",
    "            if pd.isna(X.at[idx, 'latitude']):\n",
    "                val_lat = np.nan\n",
    "                if (st_k is not pd.NA) and (ct_k is not pd.NA):\n",
    "                    try:\n",
    "                        val_lat = self.mean_latlon_by_pair_.loc[(st_k, ct_k), 'latitude']\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "                if pd.isna(val_lat):\n",
    "                    val_lat = self.global_lat_mean_\n",
    "                X.at[idx, 'latitude'] = val_lat\n",
    "\n",
    "            if pd.isna(X.at[idx, 'longitude']):\n",
    "                val_lon = np.nan\n",
    "                if (st_k is not pd.NA) and (ct_k is not pd.NA):\n",
    "                    try:\n",
    "                        val_lon = self.mean_latlon_by_pair_.loc[(st_k, ct_k), 'longitude']\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "                if pd.isna(val_lon):\n",
    "                    val_lon = self.global_lon_mean_\n",
    "                X.at[idx, 'longitude'] = val_lon\n",
    "\n",
    "        X.drop(columns=['city_key','state_key'], inplace=True, errors='ignore')\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ffd4eb7-3bea-4cd0-8486-cf0e367a34b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _geo_to_typed(X):\n",
    "    # 兼容 ndarray / DataFrame\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X, columns=geo_cols)\n",
    "    df = X.copy()\n",
    "    # 经纬度：非数字 -> NaN\n",
    "    df['latitude']  = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "    df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')\n",
    "    # 文本列：去空白，空串 -> NaN\n",
    "    for c in ['cityname', 'state']:\n",
    "        df[c] = (df[c].astype('string')\n",
    "                        .str.strip()\n",
    "                        .replace({'': pd.NA}))\n",
    "    return df\n",
    "\n",
    "geo_to_typed = FunctionTransformer(_geo_to_typed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab338cf8-a485-4d97-86e3-6d7f425b2969",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_pipe = Pipeline(steps=[\n",
    "    ('to_typed', geo_to_typed),\n",
    "    ('impute_geo', GeoCityStateImputer()),\n",
    "    ('make_city_state', FunctionTransformer(\n",
    "        lambda X: X.assign(\n",
    "            city_state=(\n",
    "                X['cityname'].astype('string').str.strip() + ', ' + X['state'].astype('string').str.strip()\n",
    "            )\n",
    "        )\n",
    "    )),\n",
    "    ('encode', ColumnTransformer(\n",
    "        transformers=[\n",
    "            # 对合并列做独热编码\n",
    "            ('ohe_city_state', OneHotEncoder(handle_unknown='infrequent_if_exist', min_frequency=0.05), ['city_state']),\n",
    "            # 经纬度保留为数值特征\n",
    "            ('pass_latlon', 'passthrough', ['latitude', 'longitude']),\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab5cac2-8a09-4879-9233-a056ea851a90",
   "metadata": {},
   "source": [
    "处理fee, has_photo和source列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b95e5cb-bbc4-4863-acf8-5aa854831526",
   "metadata": {},
   "outputs": [],
   "source": [
    "fee_photo_source_pipe = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('one_hot', OneHotEncoder(handle_unknown='infrequent_if_exist', min_frequency=0.05))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e392259d-36f5-4c66-abc6-fb7e1b9a252a",
   "metadata": {},
   "source": [
    "处理time列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df8a958c-5eaf-4a7f-bb3a-c8e6c44af401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    时间戳 -> [sin(month), cos(month), days_since_first]\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.t0_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        x = pd.to_datetime(pd.Series(np.ravel(X)), errors=\"coerce\", utc=True)\n",
    "        if x.notna().any():\n",
    "            self.t0_ = x.min()\n",
    "        else:\n",
    "            # 极端：全缺失，给个稳定兜底\n",
    "            self.t0_ = pd.Timestamp(\"1970-01-01\", tz=\"UTC\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        x = pd.to_datetime(pd.Series(np.ravel(X)), errors=\"coerce\", utc=True).fillna(self.t0_)\n",
    "        month = x.dt.month.to_numpy()\n",
    "        sin_month = np.sin(2*np.pi*month/12.0)\n",
    "        cos_month = np.cos(2*np.pi*month/12.0)\n",
    "        days_since = ((x - self.t0_).dt.total_seconds()/86400.0).to_numpy()\n",
    "        return np.vstack([sin_month, cos_month, days_since]).T\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array([\"time_sin_month\", \"time_cos_month\", \"time_days_since_first\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "293919c5-559a-47cf-a419-993eae69e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_pipe = Pipeline(steps=[\n",
    "    (\"encode\", TimeEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721d83cb-c853-48f7-a0cd-46468dc5f5db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75d96c7c-a41d-4488-95d0-87930a10f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "bed_bath_sqft_cols = ['bedrooms','bathrooms','square_feet']\n",
    "geo_cols = ['latitude', 'longitude', 'cityname', 'state']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('category', category_pipe, ['category']),\n",
    "        ('text', title_body_pipeline(title_weight=3.0), ['title', 'body']),\n",
    "        ('amenities', amenities_pipe, ['amenities']),\n",
    "        ('fee_photo_source', fee_photo_source_pipe, ['fee', 'has_photo', 'source']),\n",
    "        ('pets', pets_allowed_pipe, ['pets_allowed']),\n",
    "        ('layout', bed_bath_sqft_pipe, bed_bath_sqft_cols),\n",
    "        ('geo', geo_pipe, geo_cols),\n",
    "        ('time', time_pipe, ['time']),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e58b262-39c7-4dea-8324-100dbfcecf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipe_xgb = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('xgb', XGBRegressor(\n",
    "        objective='reg:absoluteerror',\n",
    "        eval_metric='mae',\n",
    "        subsample=0.8,\n",
    "        reg_alpha=0.1, \n",
    "        reg_lambda=5.0,\n",
    "        max_bin=128,\n",
    "        min_child_weight=6.0,\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cuda\",\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38bf71f0-e531-4ae7-872d-62ee9c223de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_xgb = {\n",
    "    'xgb__max_depth':       [2, 3, 4],\n",
    "    'xgb__n_estimators':    [2000, 2200, 2400, 2600, 2800],\n",
    "    'xgb__colsample_bytree':[0.25, 0.26, 0.27]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb0fc2b6-4090-48bc-899d-e8c18fe1e21a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_xgb = HalvingGridSearchCV(\n",
    "    estimator=full_pipe_xgb,\n",
    "    param_grid=param_grid_xgb,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    refit=True,\n",
    "    verbose=1,\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c70d0da2-9545-4d0c-bcfd-ce915d846744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据加载完成，包含 63663 条记录。\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "        \n",
    "X_train = df_train.drop(columns=['price'])\n",
    "y_train = df_train['price']\n",
    "print(f\"训练数据加载完成，包含 {len(df_train)} 条记录。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e49807e-85e2-44d2-bfcd-73618b3ac2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 4\n",
      "n_required_iterations: 4\n",
      "n_possible_iterations: 4\n",
      "min_resources_: 2357\n",
      "max_resources_: 63663\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 45\n",
      "n_resources: 2357\n",
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 15\n",
      "n_resources: 7071\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 5\n",
      "n_resources: 21213\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 2\n",
      "n_resources: 63639\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "\n",
      "✅ 训练完成！\n",
      "--------------------------------------------------\n",
      "📊 交叉验证结果:\n",
      "  - 最佳参数: {'xgb__colsample_bytree': 0.26, 'xgb__max_depth': 2, 'xgb__n_estimators': 2800}\n",
      "  - 最佳交叉验证 MAE: 203.5634\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "grid_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n✅ 训练完成！\")\n",
    "print(\"-\" * 50)\n",
    "print(\"📊 交叉验证结果:\")\n",
    "print(f\"  - 最佳参数: {grid_xgb.best_params_}\")\n",
    "print(f\"  - 最佳交叉验证 MAE: {-grid_xgb.best_score_:.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd266c14-7104-447e-b414-a22b31285bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 测试集评估：\n",
      "  - MAE : 196.9422\n",
      "  - MAPE: 12.10% \n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "if 'price' not in df_test.columns:\n",
    "    raise ValueError(\"test.csv 中未找到目标列 'price'。请确认测试集包含真实的 price 以便计算指标。\")\n",
    "\n",
    "X_test = df_test.drop(columns=['price'])\n",
    "y_test = df_test['price'].to_numpy()\n",
    "\n",
    "# 预测\n",
    "y_pred = grid_xgb.predict(X_test)\n",
    "\n",
    "# 指标：MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# 指标：MAPE（对 y_true=0 的样本进行掩码，避免除零）\n",
    "mask = y_test != 0\n",
    "if mask.sum() == 0:\n",
    "    mape = np.nan\n",
    "    mape_note = \"（所有真实值为0，MAPE无法计算）\"\n",
    "else:\n",
    "    mape = np.mean(np.abs((y_test[mask] - y_pred[mask]) / y_test[mask])) * 100\n",
    "    mape_note = \"\"\n",
    "\n",
    "print(\"🧪 测试集评估：\")\n",
    "print(f\"  - MAE : {mae:.4f}\")\n",
    "print(f\"  - MAPE: {mape:.2f}% {mape_note}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
